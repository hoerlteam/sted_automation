{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging tiled overviews in a regular grid\n",
    "\n",
    "Here we image tiled overviews in a regular grid, similar to the large image functionality of other microscopy software or the LIGHTBOX interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import specpy\n",
    "\n",
    "from autosted.callback_buildingblocks import (\n",
    "    FOVSettingsGenerator,\n",
    "    JSONSettingsLoader,\n",
    "    LocationRemover,\n",
    "    NewestDataSelector,\n",
    "    PositionListOffsetGenerator,\n",
    "    SimpleManualOffset,\n",
    "    StageOffsetsSettingsGenerator,\n",
    ")\n",
    "from autosted.detection import SimpleFocusPlaneDetector\n",
    "from autosted.imspector import get_current_stage_coords\n",
    "from autosted.pipeline import AcquisitionPipeline\n",
    "from autosted.taskgeneration import AcquisitionTaskGenerator\n",
    "from autosted.utils.tiling import centered_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save & whether to save combined HDF5 file\n",
    "save_folder = \"examples/acquisition_data/large_image_test\"\n",
    "save_hdf5 = True\n",
    "\n",
    "# path of measurement parameters (dumped to JSON file)\n",
    "# measurement_parameters = 'C:/Users/RESOLFT/Desktop/config_json/gabi/20240307_590_480_overview.json'\n",
    "# alternative: use current from Imspector\n",
    "measurement_parameters = (\n",
    "    specpy.get_application().value_at(\"\", specpy.ValueTree.Measurement).get()\n",
    ")\n",
    "\n",
    "# yx FOV size\n",
    "fov_size = [50e-6, 50e-6]\n",
    "\n",
    "# yx number of tiles\n",
    "n_tiles = [4, 4]\n",
    "\n",
    "# how much the tiles should overlap (0-1)\n",
    "overlap_fraction = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current coordinates and print, so we can go back to that position\n",
    "start_coords = get_current_stage_coords()\n",
    "print(start_coords)\n",
    "\n",
    "# generate regular grid around current stage position\n",
    "# NOTE: we add empty z-fov size and 1 tile to get 3d coordinates\n",
    "coordinate_list = centered_tiles(\n",
    "    start_coords,\n",
    "    fov_size=[0] + fov_size,\n",
    "    n_tiles=[1] + n_tiles,\n",
    "    overlap=overlap_fraction,\n",
    ")\n",
    "coordinate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline object (just one level: 'field')\n",
    "pipeline = AcquisitionPipeline(\n",
    "    save_folder, [\"field\"], save_combined_hdf5=save_hdf5, name=\"multipoint-acquisition\"\n",
    ")\n",
    "\n",
    "# callback that will create an acquisition task with given measurement parameters\n",
    "# at the next stage coordinates in the coordinate list (the next 'position')\n",
    "next_position_generator = AcquisitionTaskGenerator(\n",
    "    \"field\",\n",
    "    LocationRemover(JSONSettingsLoader(measurement_parameters)),\n",
    "    PositionListOffsetGenerator(coordinate_list),\n",
    "    FOVSettingsGenerator(lengths=[None] + fov_size),\n",
    ")\n",
    "\n",
    "# attach callback so that after each position, the next one will be enqueued\n",
    "pipeline.add_callback(next_position_generator, \"field\")\n",
    "\n",
    "# start with initial task from callback\n",
    "pipeline.run(next_position_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With autofocus and adjustable FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel to focus in\n",
    "focus_channel = 0\n",
    "\n",
    "# manual offset (zyx) to focus\n",
    "manual_focus_offset = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline object (just one level: 'field')\n",
    "pipeline = AcquisitionPipeline(\n",
    "    save_folder, [\"field\"], save_combined_hdf5=save_hdf5, name=\"multipoint-acquisition\"\n",
    ")\n",
    "\n",
    "# callback that will create an acquisition task with given measurement parameters\n",
    "# at the next stage coordinates in the coordinate list (the next 'position')\n",
    "next_position_generator = AcquisitionTaskGenerator(\n",
    "    \"field\",\n",
    "    LocationRemover(JSONSettingsLoader(measurement_parameters)),\n",
    "    PositionListOffsetGenerator(coordinate_list),\n",
    "    FOVSettingsGenerator(lengths=[None] + fov_size),\n",
    "    StageOffsetsSettingsGenerator(\n",
    "        SimpleManualOffset(\n",
    "            SimpleFocusPlaneDetector(\n",
    "                NewestDataSelector(pipeline, level=\"field\"), channel=focus_channel\n",
    "            ),\n",
    "            offset=manual_focus_offset,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# attach callback so that after each position, the next one will be enqueued\n",
    "pipeline.add_callback(next_position_generator, \"field\")\n",
    "\n",
    "# start with initial task from callback\n",
    "pipeline.run(next_position_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stitch acquired data\n",
    "\n",
    "Here, we use registration and fusion functionality from ```calmutils.stitching``` (that is also used internally by e.g. the on-the-fly stitching of autosted) to stitch the acquired tiled images into one large image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted.utils.parameter_constants import DIRECTION_STAGE, PIXEL_SIZE_PARAMETERS\n",
    "from autosted.utils.dict_utils import get_parameter_value_array_from_dict\n",
    "from calmutils.stitching import stitch\n",
    "from calmutils.stitching.fusion import fuse_image\n",
    "from calmutils.stitching.transform_helpers import translation_matrix\n",
    "\n",
    "# index of flipped axes\n",
    "flip_axes = [i for i, d in enumerate(DIRECTION_STAGE) if d < 0]\n",
    "\n",
    "# NOTE: stage sirection may not correspond to top-left to bottom-right of images\n",
    "# generate dummy regular grid around current stage position with flipped coordinates\n",
    "coordinate_list_for_stitch = centered_tiles(\n",
    "    start_coords,\n",
    "    fov_size=[0] + fov_size,\n",
    "    n_tiles=[1] + n_tiles,\n",
    "    overlap=overlap_fraction,\n",
    "    flip_axes=flip_axes,\n",
    ")\n",
    "\n",
    "# get images of a particular channel and configuration\n",
    "configuration = 0\n",
    "channel = 0\n",
    "images = [v.data[configuration][channel].squeeze() for v in pipeline.data.values()]\n",
    "\n",
    "is2d = images[0].ndim == 2\n",
    "\n",
    "# get pixel size\n",
    "settings = pipeline.data[(0,)].measurement_settings[configuration]\n",
    "pixel_sizes = get_parameter_value_array_from_dict(settings, PIXEL_SIZE_PARAMETERS)\n",
    "\n",
    "# build (pixel-unit) transform matrix from coordinates\n",
    "transforms = [\n",
    "    translation_matrix((c / pixel_sizes)[(1 if is2d else 0) :])\n",
    "    for c in coordinate_list_for_stitch\n",
    "]\n",
    "\n",
    "# fuse into one image\n",
    "fused = fuse_image(images, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative: with registration\n",
    "transforms = stitch(\n",
    "    images,\n",
    "    [(c / pixel_sizes)[(1 if is2d else 0) :] for c in coordinate_list_for_stitch],\n",
    "    corr_thresh=0.9,\n",
    ")\n",
    "\n",
    "# fuse into one image\n",
    "fused = fuse_image(images, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# show stitched image\n",
    "# NOTE: if you have 3D data, do a projection or show using napari\n",
    "plt.imshow(fused, cmap=\"magma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosted-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
