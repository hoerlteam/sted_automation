{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-step pyramidal imaging\n",
    "\n",
    "In this notebook, we implement a pyramidal, Google Maps-like, imaging scheme: We start with a coarse image with large pixel size, divide it into tiles and then only image tiles that contain at least some structure at higher resolution (smaller pixel size, though increasing STED power could also be implemented).\n",
    "\n",
    "First, we define a function that investigates an image and returns bounding boxes of sub-images that should be imaged at higher resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import specpy\n",
    "from itertools import pairwise, product\n",
    "\n",
    "\n",
    "def divide_into_tiles(img, num_divisions=2, threshold=0, min_size_fraction=0.05):\n",
    "    \"\"\"\n",
    "    Divide an image into tiles and check if a minimum of tile area is above a set threshold.\n",
    "    Return bounding boxes for each tile that matches this criterion.\n",
    "    \"\"\"\n",
    "\n",
    "    # num_divisions can be sequence (per dimension)\n",
    "    # if only a scalar is given, we re-use it for all dimensions\n",
    "    if np.isscalar(num_divisions):\n",
    "        num_divisions = [num_divisions] * img.ndim\n",
    "\n",
    "    # get all candidate bboxes\n",
    "    bboxes = []\n",
    "    start_end_per_dimension = [\n",
    "        map(list, pairwise(np.linspace(0, s, n + 1)))\n",
    "        for s, n in zip(img.shape, num_divisions)\n",
    "    ]\n",
    "    for bbox in map(list, product(*start_end_per_dimension)):\n",
    "        bboxes.append(np.array(bbox).T)\n",
    "\n",
    "    # select only the ones with enough intensity in img\n",
    "    bboxes_above_thresh = []\n",
    "    for bbox in bboxes:\n",
    "        # select tile defined by bbox rounded to nearest pixel\n",
    "        start, end = bbox\n",
    "        start = np.round(start).astype(int)\n",
    "        end = np.round(end).astype(int)\n",
    "        tile = img[tuple(slice(s, e) for s, e in zip(start, end))]\n",
    "        # if fraction of pixels above threshold is above minimal size fraction,\n",
    "        # accept into final list of bboxes\n",
    "        if (tile >= threshold).sum() / tile.size >= min_size_fraction:\n",
    "            bboxes_above_thresh.append(bbox.ravel())\n",
    "\n",
    "    return bboxes_above_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our function, with Imspector running in the background, let's apply it to the currently open image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = specpy.get_application()\n",
    "\n",
    "channel = 0\n",
    "img = im.active_measurement().stack(channel).data().squeeze()\n",
    "\n",
    "divide_into_tiles(img, threshold=4, num_divisions=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration into a **autoSTED** pipeline\n",
    "\n",
    "To integrate this function into an automated imaging pipeline, all we have to do is wrap it in a ```ROIDetectorWrapper``` in a callback that enqueuse higher-resolution sub-images.\n",
    "\n",
    "**Note:** we only image one region (```start_callback``` is only called once as the initial callback to ```pipeline.run()```). By adding e.g. a ```SpiralOffsetGenerator``` and calling it after each overview, this could be easily extended to larger-scale imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted import AcquisitionPipeline\n",
    "from autosted.taskgeneration import AcquisitionTaskGenerator\n",
    "from autosted.callback_buildingblocks import (\n",
    "    JSONSettingsLoader,\n",
    "    FOVSettingsGenerator,\n",
    "    LocationRemover,\n",
    ")\n",
    "from autosted.detection import ROIDetectorWrapper\n",
    "from autosted.utils.dict_utils import get_parameter_value_array_from_dict\n",
    "from autosted.utils.parameter_constants import PIXEL_SIZE_PARAMETERS\n",
    "\n",
    "# get current measurement from Imspector\n",
    "im = specpy.get_application()\n",
    "params = im.value_at(\"\", specpy.ValueTree.Measurement).get()\n",
    "\n",
    "# get pixel size of active measurement\n",
    "pixel_size = get_parameter_value_array_from_dict(params, PIXEL_SIZE_PARAMETERS)\n",
    "\n",
    "pipeline = AcquisitionPipeline(\"acquisition_data/pyramid\", [\"level0\", \"level1\"])\n",
    "\n",
    "# initial image at double pixel size (i.e. half resolution)\n",
    "start_callback = AcquisitionTaskGenerator(\n",
    "    \"level0\",\n",
    "    LocationRemover(JSONSettingsLoader(params)),\n",
    "    FOVSettingsGenerator(pixel_sizes=pixel_size * 2),\n",
    ")\n",
    "\n",
    "# get ROIs to image at level1 with original pixel size using divide_into_tiles\n",
    "tile_callback = AcquisitionTaskGenerator(\n",
    "    \"level1\",\n",
    "    LocationRemover(JSONSettingsLoader(params)),\n",
    "    ROIDetectorWrapper(divide_into_tiles, detection_kwargs={\"threshold\": 4}),\n",
    ")\n",
    "\n",
    "pipeline.add_callback(tile_callback, \"level0\")\n",
    "pipeline.run(start_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to multiple levels\n",
    "\n",
    "Above, we run a two-level pipeline.\n",
    "\n",
    "There is no reason to stop there - by simply adding another hierarchy level in the pipeline and a callback to sub-divide the images of the second level, we can add a third level (or more).\n",
    "\n",
    "Note, that by default acquisition tasks of a later level take priority, thus our image pyramid would be scanned in a depth-first manner. This can be changed by assigning custom priorities for the levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = specpy.get_application()\n",
    "params = im.value_at(\"\", specpy.ValueTree.Measurement).get()\n",
    "\n",
    "pixel_size = get_parameter_value_array_from_dict(params, PIXEL_SIZE_PARAMETERS)\n",
    "\n",
    "pipeline = AcquisitionPipeline(\n",
    "    \"acquisition_data/pyramid\", [\"level0\", \"level1\", \"level2\"]\n",
    ")\n",
    "\n",
    "# by default, increasing levels have a lower priority number, i.e. they will be imaged first\n",
    "# by giving them increasing priorities, we can instead do a breadth-first traversal\n",
    "pipeline.level_priorities = {\"level0\": 0, \"level1\": 1, \"level2\": 2}\n",
    "\n",
    "# first overview at 9-fold subsampling\n",
    "start_callback = AcquisitionTaskGenerator(\n",
    "    \"level0\",\n",
    "    LocationRemover(JSONSettingsLoader(params)),\n",
    "    FOVSettingsGenerator(pixel_sizes=pixel_size * 9),\n",
    ")\n",
    "\n",
    "# second level at 3-fold subsampling\n",
    "tile_callback_l1 = AcquisitionTaskGenerator(\n",
    "    \"level1\",\n",
    "    LocationRemover(JSONSettingsLoader(params)),\n",
    "    FOVSettingsGenerator(pixel_sizes=pixel_size * 3),\n",
    "    ROIDetectorWrapper(\n",
    "        divide_into_tiles, detection_kwargs={\"threshold\": 5, \"num_divisions\": 3}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 3rd level at original pixel size\n",
    "tile_callback_l2 = AcquisitionTaskGenerator(\n",
    "    \"level2\",\n",
    "    LocationRemover(JSONSettingsLoader(params)),\n",
    "    ROIDetectorWrapper(\n",
    "        divide_into_tiles, detection_kwargs={\"threshold\": 5, \"num_divisions\": 3}\n",
    "    ),\n",
    ")\n",
    "\n",
    "pipeline.add_callback(tile_callback_l1, \"level0\")\n",
    "pipeline.add_callback(tile_callback_l2, \"level1\")\n",
    "pipeline.run(start_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing acquired data\n",
    "\n",
    "Here, we use image fusion functionality from ```calmutils``` to stitch the images of a given pyramid level into one large image.\n",
    "\n",
    "We make use of the helper function ```approximate_pixel_shift_from_settings``` to get a virtual position of the images, i.e. combining scan and stage offsets (approximate due to rounding to nearest integer pixel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted.utils.coordinate_utils import approximate_pixel_shift_from_settings\n",
    "from calmutils.stitching.transform_helpers import translation_matrix\n",
    "from calmutils.stitching.fusion import fuse_image\n",
    "\n",
    "level = \"level2\"\n",
    "channel = 0\n",
    "configuration = 0\n",
    "\n",
    "# get all measurement settings and images at selected level, config and channel\n",
    "settings = [\n",
    "    measurement.measurement_settings[configuration]\n",
    "    for idx, measurement in pipeline.data.items()\n",
    "    if idx[-1][0] == level\n",
    "]\n",
    "images = [\n",
    "    measurement.data[configuration][channel].squeeze()\n",
    "    for idx, measurement in pipeline.data.items()\n",
    "    if idx[-1][0] == level\n",
    "]\n",
    "\n",
    "# get pixel shifts of all images relative to first\n",
    "pixel_shifts = [\n",
    "    approximate_pixel_shift_from_settings(settings[0], setting_i)\n",
    "    for setting_i in settings\n",
    "]\n",
    "\n",
    "# to transformation matrix\n",
    "is2d = images[0].ndim == 2\n",
    "transforms = [translation_matrix(shift[(1 if is2d else 0) :]) for shift in pixel_shifts]\n",
    "\n",
    "# fuse (low out-of-bounds value to better visualize non-imaged areas)\n",
    "fused = fuse_image(images, transforms, oob_val=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be displayed using matplotlib or napari:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(fused, cmap=\"magma\", clim=(-5, 50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosted-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
