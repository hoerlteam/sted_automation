{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted import AcquisitionPipeline\n",
    "from autosted.taskgeneration import AcquisitionTaskGenerator\n",
    "from autosted.callback_buildingblocks.static_settings import JSONSettingsLoader\n",
    "from autosted.callback_buildingblocks.parameter_filtering import LocationRemover\n",
    "from autosted.callback_buildingblocks.regular_position_generators import SpiralOffsetGenerator\n",
    "from autosted.imspector import get_current_stage_coords\n",
    "from autosted.stoppingcriteria import MaximumAcquisitionsStoppingCriterion\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview-Detail imaging with **autoSTED**\n",
    "\n",
    "**Note:** Imspector should be open in the background.\n",
    "\n",
    "Let's start with what we had at the end of ```basics.ipynb``` notebook, running overview acquisitions in a spiral.\n",
    "\n",
    "Here, we added a few small changes:\n",
    "* instead of using the parameters of the current measurement, we use parameters savved to a json file via ```save_parameters_to_json.ipynb```\n",
    "* we wrap the settings loader in a ```LocationRemover```: this just removes any location-related parameters (i.e. stage/scan offsets)\n",
    "    * this is not strictly necessary here, as those parameters will be overwritten by the ```SpiralOffsetGenerator```, but to keep everything clean, it still makes sense.\n",
    "\n",
    "**Note:** When building your own pipeline, you could also build upon the other ```overview_*``` notebooks in the examples folder for:\n",
    "* imaging in a regular grid (optionally with on-the-fly stitching)\n",
    "* imaging at manually picked locations\n",
    "* image-based autofocus in the overviews\n",
    "* selective overview imaging with a pre-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline with a single overview level: \"image\" \n",
    "pipeline = AcquisitionPipeline(data_save_path='acquisition_data/test', hierarchy_levels=['image'])\n",
    "\n",
    "# path to parameters saved as JSON\n",
    "overview_config = 'config_json/test2color_overview.json'\n",
    "\n",
    "# overview generator combines settings from file with next stage positions in spiral\n",
    "next_overview_generator = AcquisitionTaskGenerator('image',\n",
    "                         LocationRemover(JSONSettingsLoader(overview_config)),\n",
    "                         SpiralOffsetGenerator(move_size=[50e-6, 50e-6], start_position=get_current_stage_coords()))\n",
    "\n",
    "# add the callback and a stopping condition\n",
    "pipeline.add_callback(next_overview_generator, 'image')\n",
    "pipeline.add_stopping_condition(MaximumAcquisitionsStoppingCriterion(5))\n",
    "\n",
    "pipeline.run(initial_callback=next_overview_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data of a run\n",
    "\n",
    "An ```AcquisitionPipeline``` stores acquired data (and the parameters used) in its ```.data``` attribute, which acts like a ```dict```.\n",
    "\n",
    "The keys are tuple of ints, a running count for each hierarchy level. E.g. the first overview image has index ```(0, )```. If we also do detail acquisitions, the second detail in the third overview would have index ```(2, 1)```, ...\n",
    "\n",
    "We can get a single ```MeasurementData``` object, which contains lists of data, measurement parameters and hardware parameters (for each *configuration* of the measurement). The data themselves are a list of NumPy arrays for the different channels of the acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data of a given index\n",
    "measurement_data = pipeline.data[(0,)]\n",
    "\n",
    "# get data of configuration 0, channel 0, squeeze singleton dimensions (Imspector stacks are always 4D)\n",
    "img = measurement_data.data[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the image with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# NOTE: this assumes you did a 2D overview\n",
    "# for 3D data, you would have to e.g., max project it along the z-axis\n",
    "# img = img.max(axis=0)\n",
    "\n",
    "plt.imshow(img, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting cells\n",
    "\n",
    "To build an automation pipeline that selectively images cells in the overview with higher resolution, we first need a segmentation function that takes an image (NumPy array) and returns an integer-valued label map of the same shape.\n",
    "\n",
    "We can use standard Python image processing functionality from libraries like ```scikit-image``` or ```scipy```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "from scipy.ndimage import gaussian_filter, label\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.morphology import dilation, disk\n",
    "\n",
    "\n",
    "def segment(img):\n",
    "    # blur and get Otsu threshold\n",
    "    g = gaussian_filter(img.astype(float), 5)\n",
    "    t = max(3, threshold_otsu(g))\n",
    "    # label connected components, remove objects at border, dilate\n",
    "    labels, _ = label(g > t)\n",
    "    labels = clear_border(labels)\n",
    "    labels = dilation(labels, disk(3))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our function on the image we got from the pipeline earlier & plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = segment(img)\n",
    "\n",
    "plt.imshow(label_map, cmap='turbo', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced segmentation\n",
    "\n",
    "The segmentation function does not have any specific dependencies to autoSTED, so you can use pretty much anything in the Python image processing ecosystem, e.g. deep learning-based segmentation via Cellpose (https://github.com/MouseLand/cellpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose.models import Cellpose\n",
    "\n",
    "# instantiate Cellpose model\n",
    "model = Cellpose(gpu=True, model_type='nuclei')\n",
    "\n",
    "def segment_cellpose(img, diameter=30, model=model):\n",
    "    # run model, return predicted instance segmentation mask     \n",
    "    masks, flows, styles, diams = model.eval([img], diameter=diameter, flow_threshold=None, channels=[0,0])\n",
    "    return masks[0]\n",
    "\n",
    "label_map = segment_cellpose(img)\n",
    "plt.imshow(label_map, cmap='turbo', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the segmentation function for an overview-detail pipeline\n",
    "\n",
    "To move from the single-level automation pipeline above to an overview-detail pipeline, we just have to add a second callback to it that will be called after each overview image and enqueue details.\n",
    "\n",
    "Again, we can construct it from simple building blocks using an ```AcquisitionTaskGenerator```:\n",
    "\n",
    "* 1. get base settings from a JSON File\n",
    "* 2. take the location of the overview image (esp. stage position)\n",
    "* 3. use our segmentation function wrapped in a ```SegmentationWrapper``` - this will apply the function to the newest image that was acquired, translate the pixel objects into scan offset & ROI size parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted.callback_buildingblocks.parameter_filtering import LocationKeeper\n",
    "from autosted.callback_buildingblocks.data_selection import NewestSettingsSelector\n",
    "from autosted.detection.roi_detection import SegmentationWrapper\n",
    "\n",
    "# pipeline and overview generator as above, but we now have two levels: 'overview', 'detail'\n",
    "pipeline = AcquisitionPipeline(data_save_path='acquisition_data/test', hierarchy_levels=['overview', 'detail'])\n",
    "\n",
    "overview_config = 'config_json/test2color_overview.json'\n",
    "next_overview_generator = AcquisitionTaskGenerator('overview',\n",
    "                         JSONSettingsLoader(overview_config),\n",
    "                         SpiralOffsetGenerator(move_size=[50e-6, 50e-6], start_position=get_current_stage_coords()))\n",
    "\n",
    "# acquisition task generator for details as described above\n",
    "detail_config = \"config_json/test2color_detail.json\"\n",
    "detail_generator = AcquisitionTaskGenerator(\n",
    "    \"detail\",\n",
    "    # 1. base settings from file\n",
    "    LocationRemover(JSONSettingsLoader(detail_config)),\n",
    "    # 2. locations (stage) from previous (overview) image\n",
    "    LocationKeeper(NewestSettingsSelector()),\n",
    "    # 3. segmentation wrapper around segmentation function\n",
    "    SegmentationWrapper(segment),\n",
    ")\n",
    "\n",
    "pipeline.add_callback(next_overview_generator, \"overview\")\n",
    "pipeline.add_callback(detail_generator, \"overview\")\n",
    "\n",
    "pipeline.add_stopping_condition(\n",
    "    # instead of a maximum of total images, we can also specify a maximum per level\n",
    "    MaximumAcquisitionsStoppingCriterion(\n",
    "        max_acquisitions_per_level={\"overview\": 5, \"detail\": 20}\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline.run(initial_callback=next_overview_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SegmentationWrapper Internals\n",
    "\n",
    "The SegmentationWrapper object takes care of getting image data as NumPy array(s), passing it to our segmentation function and translating the pixel results back to physical microscope parameters (scan offsets, FOV lengths) and wrapping them as parameter ```dict```s.\n",
    "\n",
    "We can adjust the behavious via the constructor, e.g.:\n",
    "* how to get the measurement data in which to perform detection (by default, we select the newest image of the level at which the callback is attached)\n",
    "* which configuration(s) and channel(s) to use\n",
    "* whether to plot results\n",
    "* wheter to return scan or stage offsets\n",
    "* whether to return a ready-to-use parameter dictionary (default) or just a list of bounding boxes, which may be isnteresting if we want to e.g., manually add an offset. In the latter case, we can nest the SegmentationWrapper in a ```ScanOffsetsSettingsGenerator``` or ```StageOffsetsSettingsGenerator``` to wrap the results into a parameter dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted.callback_buildingblocks.coordinate_value_wrappers import ScanOffsetsSettingsGenerator\n",
    "from autosted.callback_buildingblocks.data_selection import NewestDataSelector\n",
    "\n",
    "detail_generator = AcquisitionTaskGenerator(\n",
    "    \"detail\",\n",
    "    LocationRemover(JSONSettingsLoader(\"config_json/test2color_detail.json\")),\n",
    "    LocationKeeper(NewestSettingsSelector()),\n",
    "    ScanOffsetsSettingsGenerator(\n",
    "        SegmentationWrapper(\n",
    "            segment,\n",
    "            data_source_callback=NewestDataSelector(pipeline=pipeline, level=\"overview\"),\n",
    "            configurations=0,\n",
    "            channels=0,\n",
    "            offset_parameters='scan',\n",
    "            plot_detections=True,\n",
    "            return_parameter_dict=False            \n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding on-the-fly stitching of overviews\n",
    "\n",
    "The modular nature of autoSTED allows for easy exchange of building blocks, e.g. the data source callback of our segmentation wrapper could be exchanged for a ```StitchedNewestDataSelector``` to virtually stitch overview images and thus prevent skipping of cells on the border of overview tiles.\n",
    "\n",
    "One issue that might arise here is that since the cell detector \"sees the same overview multiple times\" cells might be selected for detailled imagin multiple times. To prevent this, we have an ```AlreadyImagedFOVFilter``` that can be attached to the acquisition task generator to skip already imaged FOVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosted.taskgeneration.task_filtering import AlreadyImagedFOVFilter\n",
    "from autosted.callback_buildingblocks.stitched_data_selection import StitchedNewestDataSelector\n",
    "\n",
    "pipeline = AcquisitionPipeline(\n",
    "    data_save_path=\"acquisition_data/test\", hierarchy_levels=[\"overview\", \"detail\"]\n",
    ")\n",
    "\n",
    "overview_config = 'config_json/test2color_overview.json'\n",
    "next_overview_generator = AcquisitionTaskGenerator(\n",
    "    \"overview\",\n",
    "    JSONSettingsLoader(overview_config),\n",
    "    SpiralOffsetGenerator(\n",
    "        # NOTE: smaller move size to have overlap\n",
    "        move_size=[40e-6, 40e-6], start_position=get_current_stage_coords()\n",
    "    ),\n",
    ")\n",
    "\n",
    "detail_config = \"config_json/test2color_detail.json\"\n",
    "detail_generator = AcquisitionTaskGenerator(\n",
    "    \"detail\",\n",
    "    LocationRemover(JSONSettingsLoader(detail_config)),\n",
    "    LocationKeeper(NewestSettingsSelector(pipeline, \"overview\")),\n",
    "    SegmentationWrapper(\n",
    "        segment,\n",
    "        # instead of default NewestDataSelector, we use StitchedNewestDataSelector\n",
    "        # which returns a virtually stitched image of the most recent overview and its neighbors\n",
    "        data_source_callback=StitchedNewestDataSelector(\n",
    "            pipeline, \"overview\", register_tiles=False, offset_parameters=\"scan\"\n",
    "        ),\n",
    "        offset_parameters=\"scan\",\n",
    "    ),\n",
    ")\n",
    "# we add a task filter to ignore FOVs already imaged at \"detail\" level\n",
    "detail_generator.add_task_filters(AlreadyImagedFOVFilter(pipeline, \"detail\", 0.5, True))\n",
    "\n",
    "\n",
    "pipeline.add_callback(next_overview_generator, \"overview\")\n",
    "pipeline.add_callback(detail_generator, \"overview\")\n",
    "\n",
    "pipeline.add_stopping_condition(\n",
    "    MaximumAcquisitionsStoppingCriterion(\n",
    "        max_acquisitions_per_level={\"overview\": 5, \"detail\": 20}\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline.run(initial_callback=next_overview_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
